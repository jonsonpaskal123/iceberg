# فاز ۲: اجرای یک خط لوله ETL ساده

**هدف:** در این فاز، یک اسکریپت Spark را اجرا می‌کنیم تا داده‌های نمونه `persons` را از Elasticsearch بخواند و آن‌ها را به صورت فایل‌های Parquet در یک باکت مشخص در MinIO ذخیره کند.

---

## پیش‌نیازها

۱. سرویس‌های زیرساخت (Elasticsearch, Kibana, MinIO, Nessie) با استفاده از `docker-compose-infra.yml` در حال اجرا هستند.
۲. ایندکس `persons` به همراه ۱۰ رکورد نمونه در Elasticsearch ایجاد شده است.

---

## گام ۱: اجرای خط لوله Spark

در همین پوشه، فایل `docker-compose-spark.yml` قرار دارد. این فایل شامل سرویس‌های Spark و یک سرویس `spark-submitter` است که اسکریپت `main.py` را اجرا می‌کند.

برای راه‌اندازی سرویس‌های Spark و اجرای خط لوله، دستور زیر را در ترمینال اجرا کنید:

```bash
docker-compose -f docker-compose-spark.yml up
```

**نکته:** پس از اجرای موفقیت‌آمیز، این سرویس به طور خودکار متوقف می‌شود.

---

## گام ۲: بررسی نتایج

*   **در کنسول:** خروجی اجرای `docker-compose` باید نشان دهد که ۱۰ رکورد با موفقیت خوانده و در MinIO نوشته شده‌اند.
*   **در MinIO:** به رابط کاربری MinIO (`http://localhost:9001`) بروید. باید یک باکت جدید به نام `phase-2-warehouse` ببینید. داخل این باکت، پوشه‌ای به نام `persons_data` وجود دارد که حاوی فایل‌های Parquet است.
