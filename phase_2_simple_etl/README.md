# فاز ۲: اجرای یک خط لوله ETL ساده

**هدف:** در این فاز، یک اسکریپت Spark را اجرا می‌کنیم تا داده‌های نمونه `persons` را از Elasticsearch بخواند و آن‌ها را به صورت فایل‌های Parquet در یک باکت مشخص در MinIO ذخیره کند.

---

## پیش‌نیازها

قبل از شروع این فاز، مطمئن شوید که **فاز ۱** را به طور کامل انجام داده‌اید:

1.  تمام سرویس‌ها با استفاده از `docker-compose` در حال اجرا هستند.
2.  ایندکس `persons` به همراه ۱۰ رکورد نمونه در Elasticsearch ایجاد شده است.

---

## گام ۱: اجرای اسکریپت Spark

اسکریپت اصلی این فاز در فایل `main.py` قرار دارد. این اسکریپت:

1.  به Spark متصل می‌شود.
2.  یک باکت به نام `phase-2-warehouse` در MinIO ایجاد می‌کند (اگر از قبل وجود نداشته باشد).
3.  داده‌ها را از ایندکس `persons` در Elasticsearch می‌خواند.
4.  داده‌ها را با فرمت Parquet در مسیر `persons_data` داخل باکت `phase-2-warehouse` ذخیره می‌کند.

برای اجرای اسکریپت، دستور زیر را در ترمینال اجرا کنید:

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.elasticsearch:elasticsearch-spark-30_2.12:8.4.3 \
  /opt/spark/work-dir/main.py
```

**نکته:** در این مرحله، ما فقط از پکیج `elasticsearch-spark` استفاده می‌کنیم، زیرا هنوز با Iceberg و Nessie کاری نداریم.

---

## گام ۲: بررسی نتایج

پس از اجرای موفقیت‌آمیز اسکریپت، نتایج را به صورت زیر بررسی کنید:

*   **در کنسول:** خروجی اجرای Spark باید نشان دهد که ۱۰ رکورد با موفقیت خوانده و در MinIO نوشته شده‌اند.
*   **در MinIO:** به رابط کاربری MinIO (`http://localhost:9001`) بروید. باید یک باکت جدید به نام `phase-2-warehouse` ببینید. داخل این باکت، پوشه‌ای به نام `persons_data` وجود دارد که حاوی فایل‌های Parquet است.

```