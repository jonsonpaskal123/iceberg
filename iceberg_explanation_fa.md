# Apache Iceberg چیست؟

آیسبرگ: یک فرمت جدول متن‌باز است که برای مدیریت مجموعه داده‌های تحلیلی در مقیاس بزرگ طراحی شده است. این به عنوان یک لایه بر روی ذخیره‌سازی دریاچه داده، مانند Google Cloud Storage، عمل می‌کند و قابلیت اطمینان و سادگی جداول SQL را به داده‌های بزرگ می‌آورد. این به موتورهای پردازش داده‌های مختلف مانند Spark، Trino، Flink، Presto و Hive اجازه می‌دهد تا با داده‌های یکسان به طور همزمان و ایمن کار کنند.

## ویژگی‌های کلیدی Apache Iceberg

*   **تکامل طرح‌واره (Schema Evolution):** این فرمت به شما امکان می‌دهد ستون‌ها را در یک جدول اضافه، حذف یا تغییر نام دهید بدون اینکه نیازی به بازنویسی کل مجموعه داده باشد. این کار مدیریت داده‌ها را ساده کرده و از مهاجرت‌های پرهزینه داده جلوگیری می‌کند.

*   **سفر در زمان و بازگشت (Time Travel and Rollback):** این فرمت تاریخچه‌ای از snapshotهای جدول را نگه می‌دارد و به کاربران امکان می‌دهد داده‌ها را در یک نقطه زمانی خاص پرس‌وجو کنند. این برای حسابرسی، اشکال‌زدایی و اصلاح خطاها با بازگشت به حالت خوب قبلی مفید است.

*   **پارتیشن‌بندی پنهان (Hidden Partitioning):** آیسبرگ پارتیشن‌بندی داده‌ها را به طور خودکار مدیریت می‌کند. این بدان معناست که کاربران برای نوشتن پرس‌وجوهای کارآمد نیازی به دانستن طرح فیزیکی جدول ندارند و طرح پارتیشن را می‌توان با تغییر داده‌ها یا الگوهای پرس‌وجو به‌روزرسانی کرد.

*   **سازگاری تراکنشی (ACID):** آیسبرگ تضمین می‌کند که عملیات داده اتمی، سازگار، ایزوله و بادوام (ACID) باشد. این از خرابی و ناهماهنگی داده‌ها در هنگام خواندن و نوشتن همزمان چندین برنامه در یک جدول جلوگیری می‌کند.

*   **بهبود عملکرد پرس‌وجو (Improved Query Performance):** با استفاده از فراداده برای ردیابی فایل‌های داده، برنامه‌ریزی پرس‌وجوی Iceberg می‌تواند به طور موثر فایل‌های غیرضروری را حذف کند، که به طور قابل توجهی اجرای پرس‌وجو را در مجموعه داده‌های بزرگ سرعت می‌بخشد.

*   **سازگاری بین موتورها (Cross-Engine Compatibility):** این برای سازگاری با طیف گسترده‌ای از موتورهای پرس‌وجو و چارچوب‌های پردازشی که معمولاً در دریاچه‌های داده استفاده می‌شوند، طراحی شده است.

## مفاهیم کلیدی دیگر در آیسبرگ

علاوه بر ویژگی‌های اصلی، درک مفاهیم زیر برای کار با آیسبرگ ضروری است:

**۱. کاتالوگ (Catalog)**

*   **نقش کاتالوگ:** کاتالوگ در آیسبرگ مانند یک دفترچه تلفن برای جداول شما عمل می‌کند. وظیفه‌اش این است که نام یک جدول (مثلاً `db.sales`) را بگیرد و بگوید فراداده (metadata) فعلی آن جدول در کجا ذخیره شده است. بدون کاتالوگ، موتورهای پردازشی مانند اسپارک نمی‌دانند از کجا باید شروع به خواندن جدول کنند.
*   **انواع کاتالوگ:** آیسبرگ از کاتالوگ‌های مختلفی پشتیبانی می‌کند، از جمله:
    *   **Hive Metastore:** یکی از رایج‌ترین گزینه‌ها.
    *   **AWS Glue Catalog:** برای کسانی که در اکوسیستم AWS کار می‌کنند.
    *   **JDBC:** برای استفاده از یک پایگاه داده رابطه‌ای به عنوان کاتالوگ.
    *   **Hadoop:** که فراداده را در یک دایرکتوری در HDFS یا S3 ذخیره می‌کند.

**۲. ایندکس‌گذاری و پرش از داده (Indexing and Data Skipping)**

*   **عدم وجود ایندکس سنتی:** آیسبرگ مانند یک پایگاه داده سنتی، روی ستون‌ها ایندکس B-Tree نمی‌سازد.
*   **پرش از داده (Data Skipping):** به جای ایندکس، آیسبرگ از یک روش هوشمندانه به نام "پرش از داده" استفاده می‌کند. وقتی داده‌ها نوشته می‌شوند، آیسبرگ برای هر فایل داده، مقادیر حداقل (min) و حداکثر (max) هر ستون را در فراداده (metadata) خود ذخیره می‌کند.
*   **چگونه کار می‌کند؟** وقتی شما یک کوئری با فیلتر می‌زنید (مثلاً `WHERE price > 500`)، موتور پردازشی اول به فراداده آیسبرگ نگاه می‌کند. اگر مقدار حداکثر ستون `price` در یک فایل داده کمتر از `500` باشد، موتور پردازشی آن فایل را **اصلاً نمی‌خواند**. این کار باعث می‌شود فقط فایل‌های مرتبط بررسی شوند و سرعت کوئری‌ها به شدت بالا برود.

**۳. جستجو و کوئری (Search and Querying)**

*   **جستجو از طریق کوئری:** "جستجو" در آیسبرگ از طریق کوئری‌های SQL با شرط‌های `WHERE` انجام می‌شود. این روش برای تحلیل داده‌های ساختاریافته (structured data) طراحی شده است.
*   **تفاوت با موتورهای جستجو:** نباید آیسبرگ را با یک موتور جستجوی متنی مانند **Elasticsearch** اشتباه گرفت. Elasticsearch برای جستجوی سریع در متن‌های بدون ساختار (full-text search) بهینه شده است، در حالی که آیسبرگ برای کوئری‌های تحلیلی پیچیده روی داده‌های حجیم و ساختاریافته (analytical queries) طراحی شده است.

---

## تاریخچه

آیسبرگ: که در ابتدا توسط Netflix و Apple برای رفع محدودیت‌های Apache Hive توسعه داده شد، اکنون یک پروژه سطح بالای Apache است. این در تولید توسط شرکت‌های بزرگ فناوری برای مدیریت مجموعه داده‌های عظیم استفاده می‌شود.

## آیا با Iceberg می‌توان Lakehouse ساخت؟

پاسخ: بله، دقیقاً. Iceberg یکی از اجزای اصلی و بنیادی برای ساخت یک معماری Lakehouse به حساب می‌آید.

معماری Lakehouse در ساده‌ترین شکل خود، ترکیبی از Data Lake و Data Warehouse است:

*   **دیتا لیک (Data Lake):** داده‌ها همچنان در فرمت‌های باز (مانند Parquet) روی ذخیره‌سازی‌های ارزان (مانند S3 یا GCS) ذخیره می‌شوند.
*   **دیتا ورهاوس (Data Warehouse):** در اینجا نقش Iceberg پررنگ می‌شود. Iceberg آن لایه مدیریتی و فراداده (metadata) را اضافه می‌کند که قابلیت‌های یک انبار داده را به دریاچه داده می‌آورد:
    *   **تراکنش‌های ACID:** مانند یک پایگاه داده، از خرابی و ناهماهنگی داده‌ها جلوگیری می‌کند.
    *   **تکامل اسکیمای پویا (Schema Evolution):** به شما اجازه می‌دهد ساختار جدول را بدون بازنویسی کل داده‌ها تغییر دهید.
    *   **سفر در زمان (Time Travel):** امکان دسترسی به نسخه‌های قبلی داده‌ها را فراهم می‌کند.

بنابراین، Iceberg با افزودن قابلیت اطمینان، مدیریت تراکنش و ساختاردهی، Data Lake شما را به یک Lakehouse مدرن و کارآمد تبدیل می‌کند.

## سناریوی عملی: از ورود داده تا ذخیره‌سازی در Lakehouse

برای درک بهتر، یک سناریوی واقعی از تحلیل کلیک‌های کاربران در یک فروشگاه آنلاین را بررسی می‌کنیم:

**منبع داده:** رویدادهای کلیک کاربران که از یک وب اپلیکیشن ارسال می‌شود.

---

**مرحله ۱: ورود داده (Data Ingestion)**

1.  **رویداد (Event):** کاربری روی یک محصول در سایت کلیک می‌کند.
2.  **تولید داده:** این کلیک، یک رویداد (event) حاوی اطلاعاتی مانند `ID کاربر`، `ID محصول`، `زمان کلیک` و `نوع صفحه` ایجاد می‌کند.
3.  **جریان داده:** این رویدادها به صورت جریانی (stream) به یک سیستم مدیریت پیام مانند **Apache Kafka** ارسال می‌شوند. کافکا مانند یک صف ورودی عمل کرده و رویدادها را به ترتیب نگه می‌دارد.

**مرحله ۲: پردازش داده (Data Processing)**

1.  **خواندن از کافکا:** یک برنامه **Apache Spark** که به صورت استریم کار می‌کند، داده‌های جدید را به محض ورود از کافکا می‌خواند.
2.  **پاکسازی و تبدیل (Transformation):** داده‌های خام ممکن است نیاز به پاکسازی یا تغییر داشته باشند. اسپارک این کارها را انجام می‌دهد:
    *   **پاکسازی:** رویدادهای تکراری یا نامعتبر را حذف می‌کند.
    *   **غنی‌سازی (Enrichment):** با استفاده از `ID محصول`، نام و قیمت محصول را از یک پایگاه داده دیگر خوانده و به رویداد اضافه می‌کند.
3.  **آماده‌سازی برای ذخیره:** داده‌های پاکسازی‌شده اکنون آماده ذخیره در Lakehouse هستند.

**مرحله ۳: ذخیره‌سازی در جدول Iceberg**

1.  **نوشتن با اسپارک:** برنامه اسپارک داده‌های آماده‌شده را مستقیماً در یک جدول **Iceberg** به نام `clicks` می‌نویسد.
2.  **عملکرد Iceberg:** هنگام نوشتن، Iceberg این کارها را انجام می‌دهد:
    *   **ایجاد فایل‌های داده:** داده‌ها را به شکل فایل‌های `Parquet` (که بسیار کارآمد هستند) در Data Lake (مانند AWS S3) ذخیره می‌کند.
    *   **به‌روزرسانی فراداده (Metadata):** یک `snapshot` جدید ایجاد می‌کند که به فایل‌های Parquet جدید اشاره دارد. تمام این فرآیند به صورت یک **تراکنش ACID** انجام می‌شود؛ یعنی یا همه چیز با موفقیت انجام می‌شود یا هیچ تغییری اعمال نمی‌شود تا از خرابی داده جلوگیری شود.
    *   **مدیریت Schema:** اگر ساختار داده تغییر کرده باشد (مثلاً ستون قیمت اضافه شده باشد)، Iceberg این تغییر را در فراداده خود ذخیره می‌کند بدون اینکه به داده‌های قدیمی آسیبی برسد.

**مرحله ۴: نتیجه نهایی**

در نهایت، داده‌های کلیک کاربر به صورت امن و قابل اطمینان در Lakehouse ذخیره شده‌اند. اکنون:

*   **تیم‌های تحلیل داده:** می‌توانند با استفاده از موتورهای مختلف (مانند Trino یا Spark SQL) روی جدول `clicks` کوئری بزنند و گزارش تهیه کنند.
*   **دانشمندان داده:** می‌توانند با استفاده از قابلیت سفر در زمان، رفتار کاربران را در گذشته تحلیل کنند.
*   **مهندسین داده:** می‌توانند به راحتی اسکیمای جدول را بدون نگرانی از خرابی داده به‌روز کنند.

---

این سناریو نشان می‌دهد که چگونه Iceberg لایه "مدیریت" و "اطمینان" را به Data Lake اضافه کرده و آن را به یک Lakehouse واقعی تبدیل می‌کند.

## سناریوی دوم: انتقال داده‌های آفلاین از Elasticsearch

این سناریو به انتقال یک مجموعه داده بزرگ و آفلاین (batch) از یک منبع مانند **Elasticsearch** به یک Lakehouse مبتنی بر Iceberg می‌پردازد.

**منبع داده:** یک ایندکس Elasticsearch به نام `historical_logs` که حاوی ترابایت‌ها لاگ از چند سال گذشته است.

**هدف:** کاهش هزینه نگهداری و افزایش سرعت کوئری‌های تحلیلی.

---

**مرحله ۱: آماده‌سازی (Preparation)**

1.  **تحلیل منبع:** ابتدا ساختار داده‌ها در Elasticsearch را بررسی می‌کنیم:
    *   **نگاشت (Mapping):** فیلدها و نوع داده آن‌ها (مانند `string`, `date`, `integer`) را شناسایی می‌کنیم.
    *   **حجم داده:** حجم کل داده‌ها را برای تخمین منابع مورد نیاز تخمین می‌زنیم.
2.  **طراحی جدول مقصد:** یک جدول Iceberg به نام `logs_archive` طراحی می‌کنیم:
    *   **اسکیما (Schema):** ستون‌های جدول را مطابق با فیلدهای Elasticsearch تعریف می‌کنیم.
    *   **پارتیشن‌بندی (Partitioning):** برای بهبود کارایی، یک استراتژی پارتیشن‌بندی انتخاب می‌کنیم، مثلاً بر اساس `سال` و `ماه` از روی فیلد زمان لاگ‌ها. این کار کوئری‌های مبتنی بر بازه‌های زمانی را سرعت می‌بخشد.

**مرحله ۲: پردازش دسته‌ای (Batch Processing)**

1.  **خواندن داده از Elasticsearch:** یک برنامه **Apache Spark** به صورت دسته‌ای (batch) ایجاد می‌کنیم تا تمام داده‌ها را از Elasticsearch بخواند.
    *   اسپارک با استفاده از یک کانکتور رسمی، داده‌ها را به صورت موازی و در بخش‌های کوچک‌تر می‌خواند تا از فشار بیش از حد به کلاستر Elasticsearch جلوگیری شود.
2.  **تبدیل داده (Data Transformation):**
    *   **تغییر فرمت:** فرمت فیلد زمان را در صورت نیاز استاندارد می‌کنیم.
    *   **پاکسازی:** فیلدهای اضافی که در Lakehouse نیازی به آن‌ها نیست را حذف می‌کنیم.
    *   **ایجاد ستون‌های پارتیشن:** از روی فیلد زمان، ستون‌های `year` و `month` را برای پارتیشن‌بندی ایجاد می‌کنیم.

**مرحله ۳: ذخیره‌سازی در جدول Iceberg**

1.  **نوشتن دسته‌ای (Bulk Write):** برنامه اسپارک، دیتافریم آماده‌شده را به صورت یک‌جا در جدول Iceberg `logs_archive` می‌نویسد.
2.  **عملکرد Iceberg:**
    *   **ایجاد فایل‌های داده:** اسپارک داده‌ها را به فایل‌های `Parquet` تبدیل کرده و بر اساس کلیدهای پارتیشن (`year`, `month`) در Data Lake (مانند HDFS یا S3) ذخیره می‌کند.
    *   **ایجاد یک Snapshot بزرگ:** در پایان عملیات، Iceberg **یک snapshot کامل** ایجاد می‌کند که شامل فراداده تمام فایل‌های ایجادشده است. این عملیات به صورت **ACID** انجام می‌شود، یعنی تا زمانی که تمام داده‌ها با موفقیت نوشته نشوند، جدول برای دیگران قابل مشاهده نخواهد بود.
    *   **کامیت نهایی:** پس از اتمام نوشتن، Iceberg فراداده جدید را کامیت می‌کند و از آن لحظه به بعد، جدول `logs_archive` با تمام داده‌های تاریخی قابل استفاده است.

**مرحله ۴: نتیجه نهایی**

اکنون تمام لاگ‌های تاریخی از Elasticsearch به یک جدول Iceberg کارآمد در Lakehouse منتقل شده‌اند.

*   **کاهش هزینه:** هزینه نگهداری داده در Data Lake بسیار کمتر از Elasticsearch است.
*   **کوئری‌های سریع‌تر:** با استفاده از پارتیشن‌بندی Iceberg، کوئری‌های تحلیلی روی لاگ‌ها به شدت سریع‌تر اجرا می‌شوند.
*   **اکوسیستم باز:** اکنون می‌توان با ابزارهای مختلف Big Data (مانند Spark، Trino، Flink) روی این داده‌ها کار کرد.
*   **آزاد‌سازی منابع Elasticsearch:** کلاستر Elasticsearch اکنون می‌تواند برای لاگ‌های جاری و جستجوهای زنده استفاده شود.

---

این سناریو نشان می‌دهد که Iceberg نه تنها برای داده‌های جریانی، بلکه برای انتقال‌های دسته‌ای و یک‌جا نیز بسیار قدرتمند است.

## سناریوی سوم: آرشیو داده‌های پرسنلی یک سازمان

این سناریو به انتقال داده‌های حساس پرسنلی از یک ایندکس **Elasticsearch** به یک Lakehouse امن و قابل تحلیل می‌پردازد.

**منبع داده:** یک ایندکس Elasticsearch به نام `personnel_records` که شامل اطلاعات کامل کارمندان (قدیمی و فعال) یک سازمان است، مانند `ID کارمند`، `نام`، `دپارتمان`، `تاریخ استخدام` و `تاریخچه حقوق`.

**هدف:** انتقال این داده‌ها به یک Lakehouse امن، کم‌هزینه و قابل تحلیل برای تیم مدیریت منابع انسانی (HR).

---

**مرحله ۱: آماده‌سازی و طراحی**

1.  **تحلیل منبع:** ساختار ایندکس `personnel_records` را بررسی می‌کنیم، با توجه ویژه به فیلدهای تودرتو (nested) مانند `تاریخچه حقوق`.
2.  **طراحی جدول مقصد:** یک جدول Iceberg به نام `employee_history` طراحی می‌کنیم.
    *   **اسکیما (Schema):** ستون‌های جدول را مطابق با فیلدهای Elasticsearch تعریف می‌کنیم. ممکن است نیاز باشد فیلدهای تودرتو را به ستون‌های جداگانه تبدیل کنیم (flattening).
    *   **پارتیشن‌بندی (Partitioning):** برای تحلیل‌های منابع انسانی، پارتیشن‌بندی بر اساس `دپارتمان` و `سال استخدام` (`hire_year`) یک انتخاب عالی است.

**مرحله ۲: پردازش دسته‌ای**

1.  **خواندن داده:** یک برنامه **Apache Spark** ایجاد می‌کنیم تا تمام داده‌ها را از ایندکس `personnel_records` بخواند.
2.  **تبدیل و امنیت (Transformation & Security):** این مرحله برای داده‌های HR بسیار مهم است:
    *   **پاکسازی:** داده‌های نامعتبر را حذف می‌کنیم.
    *   **امنیت و پوشاندن (Masking):** برای حفظ حریم شخصی، بخش‌های حساس داده‌ها مانند `کد ملی` یا `شماره حساب` را می‌پوشانیم (mask).
    *   **ایجاد ستون‌های پارتیشن:** از روی `تاریخ استخدام`، ستون `hire_year` را ایجاد می‌کنیم.

**مرحله ۳: ذخیره‌سازی در جدول Iceberg**

1.  **نوشتن دسته‌ای:** برنامه اسپارک، داده‌های پاکسازی‌شده و امن‌شده را به صورت یک‌جا در جدول Iceberg `employee_history` می‌نویسد.
2.  **عملکرد Iceberg:**
    *   **ایجاد فایل‌های داده:** داده‌ها به فایل‌های `Parquet` تبدیل شده و در Data Lake بر اساس `دپارتمان` و `hire_year` ذخیره می‌شوند.
    *   **تراکنش ACID:** تمام عملیات نوشتن در یک تراکنش ACID انجام می‌شود. این یعنی تا زمانی که انتقال داده‌ها ۱۰۰٪ موفقیت‌آمیز نباشد، هیچ تغییری در جدول نهایی اعمال نمی‌شود.
    *   **کامیت نهایی:** با پایان موفقیت‌آمیز عملیات، Iceberg یک snapshot جدید ایجاد و کامیت می‌کند.

**مرحله ۴: نتیجه نهایی**

اکنون یک آرشیو کامل، امن و کارآمد از داده‌های پرسنلی در Lakehouse داریم.

*   **کاهش هزینه:** هزینه نگهداری داده‌های تاریخی به شدت کاهش پیدا می‌کند.
*   **امنیت بالا:** داده‌های حساس با کنترل دسترسی بیشتر در Lakehouse مدیریت می‌شوند.
*   **تحلیل‌های پیشرفته HR:** تیم HR اکنون می‌تواند به راحتی تحلیل‌های تاریخی انجام دهد، مانند: "میانگین افزایش حقوق در دپارتمان‌های مختلف در ۵ سال گذشته چقدر بوده است؟"
*   **آزاد‌سازی منابع:** ایندکس Elasticsearch می‌تواند فقط برای داده‌های کارمندان فعال استفاده شود.

---