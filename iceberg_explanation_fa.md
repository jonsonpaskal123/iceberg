# Apache Iceberg چیست؟

آیسبرگ: یک فرمت جدول متن‌باز است که برای مدیریت مجموعه داده‌های تحلیلی در مقیاس بزرگ طراحی شده است. این به عنوان یک لایه بر روی ذخیره‌سازی دریاچه داده، مانند Google Cloud Storage، عمل می‌کند و قابلیت اطمینان و سادگی جداول SQL را به داده‌های بزرگ می‌آورد. این به موتورهای پردازش داده‌های مختلف مانند Spark، Trino، Flink، Presto و Hive اجازه می‌دهد تا با داده‌های یکسان به طور همزمان و ایمن کار کنند.

## ویژگی‌های کلیدی Apache Iceberg

*   **تکامل طرح‌واره (Schema Evolution):** این فرمت به شما امکان می‌دهد ستون‌ها را در یک جدول اضافه، حذف یا تغییر نام دهید بدون اینکه نیازی به بازنویسی کل مجموعه داده باشد. این کار مدیریت داده‌ها را ساده کرده و از مهاجرت‌های پرهزینه داده جلوگیری می‌کند.

*   **سفر در زمان و بازگشت (Time Travel and Rollback):** این فرمت تاریخچه‌ای از snapshotهای جدول را نگه می‌دارد و به کاربران امکان می‌دهد داده‌ها را در یک نقطه زمانی خاص پرس‌وجو کنند. این برای حسابرسی، اشکال‌زدایی و اصلاح خطاها با بازگشت به حالت خوب قبلی مفید است.

*   **پارتیشن‌بندی پنهان (Hidden Partitioning):** آیسبرگ پارتیشن‌بندی داده‌ها را به طور خودکار مدیریت می‌کند. این بدان معناست که کاربران برای نوشتن پرس‌وجوهای کارآمد نیازی به دانستن طرح فیزیکی جدول ندارند و طرح پارتیشن را می‌توان با تغییر داده‌ها یا الگوهای پرس‌وجو به‌روزرسانی کرد.

*   **سازگاری تراکنشی (ACID):** آیسبرگ تضمین می‌کند که عملیات داده اتمی، سازگار، ایزوله و بادوام (ACID) باشد. این از خرابی و ناهماهنگی داده‌ها در هنگام خواندن و نوشتن همزمان چندین برنامه در یک جدول جلوگیری می‌کند.

*   **بهبود عملکرد پرس‌وجو (Improved Query Performance):** با استفاده از فراداده برای ردیابی فایل‌های داده، برنامه‌ریزی پرس‌وجوی Iceberg می‌تواند به طور موثر فایل‌های غیرضروری را حذف کند، که به طور قابل توجهی اجرای پرس‌وجو را در مجموعه داده‌های بزرگ سرعت می‌بخشد.

*   **سازگاری بین موتورها (Cross-Engine Compatibility):** این برای سازگاری با طیف گسترده‌ای از موتورهای پرس‌وجو و چارچوب‌های پردازشی که معمولاً در دریاچه‌های داده استفاده می‌شوند، طراحی شده است.

## تاریخچه

آیسبرگ: که در ابتدا توسط Netflix و Apple برای رفع محدودیت‌های Apache Hive توسعه داده شد، اکنون یک پروژه سطح بالای Apache است. این در تولید توسط شرکت‌های بزرگ فناوری برای مدیریت مجموعه داده‌های عظیم استفاده می‌شود.

## آیا با Iceberg می‌توان Lakehouse ساخت؟

پاسخ: بله، دقیقاً. Iceberg یکی از اجزای اصلی و بنیادی برای ساخت یک معماری Lakehouse به حساب می‌آید.

معماری Lakehouse در ساده‌ترین شکل خود، ترکیبی از Data Lake و Data Warehouse است:

*   **دیتا لیک (Data Lake):** داده‌ها همچنان در فرمت‌های باز (مانند Parquet) روی ذخیره‌سازی‌های ارزان (مانند S3 یا GCS) ذخیره می‌شوند.
*   **دیتا ورهاوس (Data Warehouse):** در اینجا نقش Iceberg پررنگ می‌شود. Iceberg آن لایه مدیریتی و فراداده (metadata) را اضافه می‌کند که قابلیت‌های یک انبار داده را به دریاچه داده می‌آورد:
    *   **تراکنش‌های ACID:** مانند یک پایگاه داده، از خرابی و ناهماهنگی داده‌ها جلوگیری می‌کند.
    *   **تکامل اسکیمای پویا (Schema Evolution):** به شما اجازه می‌دهد ساختار جدول را بدون بازنویسی کل داده‌ها تغییر دهید.
    *   **سفر در زمان (Time Travel):** امکان دسترسی به نسخه‌های قبلی داده‌ها را فراهم می‌کند.

بنابراین، Iceberg با افزودن قابلیت اطمینان، مدیریت تراکنش و ساختاردهی، Data Lake شما را به یک Lakehouse مدرن و کارآمد تبدیل می‌کند.

## سناریوی عملی: از ورود داده تا ذخیره‌سازی در Lakehouse

برای درک بهتر، یک سناریوی واقعی از تحلیل کلیک‌های کاربران در یک فروشگاه آنلاین را بررسی می‌کنیم:

**منبع داده:** رویدادهای کلیک کاربران که از یک وب اپلیکیشن ارسال می‌شود.

---

**مرحله ۱: ورود داده (Data Ingestion)**

1.  **رویداد (Event):** کاربری روی یک محصول در سایت کلیک می‌کند.
2.  **تولید داده:** این کلیک، یک رویداد (event) حاوی اطلاعاتی مانند `ID کاربر`، `ID محصول`، `زمان کلیک` و `نوع صفحه` ایجاد می‌کند.
3.  **جریان داده:** این رویدادها به صورت جریانی (stream) به یک سیستم مدیریت پیام مانند **Apache Kafka** ارسال می‌شوند. کافکا مانند یک صف ورودی عمل کرده و رویدادها را به ترتیب نگه می‌دارد.

**مرحله ۲: پردازش داده (Data Processing)**

1.  **خواندن از کافکا:** یک برنامه **Apache Spark** که به صورت استریم کار می‌کند، داده‌های جدید را به محض ورود از کافکا می‌خواند.
2.  **پاکسازی و تبدیل (Transformation):** داده‌های خام ممکن است نیاز به پاکسازی یا تغییر داشته باشند. اسپارک این کارها را انجام می‌دهد:
    *   **پاکسازی:** رویدادهای تکراری یا نامعتبر را حذف می‌کند.
    *   **غنی‌سازی (Enrichment):** با استفاده از `ID محصول`، نام و قیمت محصول را از یک پایگاه داده دیگر خوانده و به رویداد اضافه می‌کند.
3.  **آماده‌سازی برای ذخیره:** داده‌های پاکسازی‌شده اکنون آماده ذخیره در Lakehouse هستند.

**مرحله ۳: ذخیره‌سازی در جدول Iceberg**

1.  **نوشتن با اسپارک:** برنامه اسپارک داده‌های آماده‌شده را مستقیماً در یک جدول **Iceberg** به نام `clicks` می‌نویسد.
2.  **عملکرد Iceberg:** هنگام نوشتن، Iceberg این کارها را انجام می‌دهد:
    *   **ایجاد فایل‌های داده:** داده‌ها را به شکل فایل‌های `Parquet` (که بسیار کارآمد هستند) در Data Lake (مانند AWS S3) ذخیره می‌کند.
    *   **به‌روزرسانی فراداده (Metadata):** یک `snapshot` جدید ایجاد می‌کند که به فایل‌های Parquet جدید اشاره دارد. تمام این فرآیند به صورت یک **تراکنش ACID** انجام می‌شود؛ یعنی یا همه چیز با موفقیت انجام می‌شود یا هیچ تغییری اعمال نمی‌شود تا از خرابی داده جلوگیری شود.
    *   **مدیریت Schema:** اگر ساختار داده تغییر کرده باشد (مثلاً ستون قیمت اضافه شده باشد)، Iceberg این تغییر را در فراداده خود ذخیره می‌کند بدون اینکه به داده‌های قدیمی آسیبی برسد.

**مرحله ۴: نتیجه نهایی**

در نهایت، داده‌های کلیک کاربر به صورت امن و قابل اطمینان در Lakehouse ذخیره شده‌اند. اکنون:

*   **تیم‌های تحلیل داده:** می‌توانند با استفاده از موتورهای مختلف (مانند Trino یا Spark SQL) روی جدول `clicks` کوئری بزنند و گزارش تهیه کنند.
*   **دانشمندان داده:** می‌توانند با استفاده از قابلیت سفر در زمان، رفتار کاربران را در گذشته تحلیل کنند.
*   **مهندسین داده:** می‌توانند به راحتی اسکیمای جدول را بدون نگرانی از خرابی داده به‌روز کنند.

---

این سناریو نشان می‌دهد که چگونه Iceberg لایه "مدیریت" و "اطمینان" را به Data Lake اضافه کرده و آن را به یک Lakehouse واقعی تبدیل می‌کند.